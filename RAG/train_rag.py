"""
TRAIN RAG SYSTEM - Script hu·∫•n luy·ªán h·ªá th·ªëng
Semantic chunking + Google Embeddings + FAISS + Gemini
"""

import logging
from pdf_loader import load_all_pdfs
from semantic_chunker import semantic_chunk
from embedding_service import EmbeddingService
from vector_store import FAISSVectorStore
from config import FAISS_INDEX_PATH, FAISS_METADATA_PATH
from pathlib import Path

# ==================== LOGGING ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    logger.info("\n" + "üöÄ"*30)
    logger.info("KH·ªûI T·∫†O H·ªÜ TH·ªêNG RAG CHO D·ªÆ LI·ªÜU DU L·ªäCH VI·ªÜT NAM")
    logger.info("üöÄ"*30 + "\n")
    
    try:
        # ========== B∆Ø·ªöC 1: LOAD PDF ==========
        logger.info("üìñ B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU T·ª™ PDF")
        logger.info("-" * 60)
        documents = load_all_pdfs()
        
        # ========== B∆Ø·ªöC 2: SEMANTIC CHUNKING ==========
        logger.info("üìñ B∆Ø·ªöC 2: SEMANTIC CHUNKING")
        logger.info("-" * 60)
        chunks = semantic_chunk(documents)
        
        # ========== B∆Ø·ªöC 3: EMBEDDING ==========
        logger.info("üìñ B∆Ø·ªöC 3: EMBEDDING CHUNKS")
        logger.info("-" * 60)
        embedding_service = EmbeddingService()
        chunk_texts = [chunk.page_content for chunk in chunks]
        embeddings = embedding_service.embed_documents(chunk_texts)
        
        # ========== B∆Ø·ªöC 4: T·∫†O VECTOR STORE ==========
        logger.info("üìñ B∆Ø·ªöC 4: T·∫†O FAISS VECTOR STORE")
        logger.info("-" * 60)
        
        metadata = [
            {
                "content": chunk.page_content,
                "metadata": chunk.metadata
            }
            for chunk in chunks
        ]
        
        vector_store = FAISSVectorStore()
        vector_store.create_index(embeddings, metadata)
        
        # ========== B∆Ø·ªöC 5: L∆ØU INDEX ==========
        logger.info("üìñ B∆Ø·ªöC 5: L∆ØU INDEX")
        logger.info("-" * 60)
        vector_store.save()
        
        # ========== HO√ÄN T·∫§T ==========
        logger.info("\n" + "‚úÖ"*30)
        logger.info("HO√ÄN T·∫§T HU·∫§N LUY·ªÜN H·ªÜ TH·ªêNG RAG!")
        logger.info("‚úÖ"*30)
        
        logger.info("\nüìä TH·ªêNG K√ä:")
        logger.info(f"  ‚Ä¢ T·ªïng PDF pages: {len(documents)}")
        logger.info(f"  ‚Ä¢ T·ªïng chunks: {len(chunks)}")
        logger.info(f"  ‚Ä¢ Embedding dimension: {embeddings.shape[1]}")
        logger.info(f"  ‚Ä¢ Vector store size: {vector_store.index.ntotal}")
        logger.info(f"\nüíæ L∆∞u t·∫°i:")
        logger.info(f"  ‚Ä¢ Index: {FAISS_INDEX_PATH}")
        logger.info(f"  ‚Ä¢ Metadata: {FAISS_METADATA_PATH}")
        
        return True
    
    except Exception as e:
        logger.error(f"\n‚ùå L·ªñI HU·∫§N LUY·ªÜN: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
