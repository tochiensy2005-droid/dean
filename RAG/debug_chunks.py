import logging
from pdf_loader import load_all_pdfs
from semantic_chunker import semantic_chunk

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

logger.info("ğŸ“š Load toÃ n bá»™ PDF documents...")
docs = load_all_pdfs()

logger.info("âœ‚ï¸  Báº¯t Ä‘áº§u semantic chunking...")
chunks = semantic_chunk(docs)

logger.info(f"\nâœ… Táº¡o {len(chunks)} chunks\n")

# Test keywords
keywords = ["PhÃº Quá»‘c", "HÃ  Ná»™i", "Du lá»‹ch", "Viá»‡t Nam", "ÄÃ  Láº¡t"]

for keyword in keywords:
    matching_chunks = [
        (i, chunk.page_content[:250], chunk.metadata) 
        for i, chunk in enumerate(chunks) 
        if keyword.lower() in chunk.page_content.lower()
    ]
    
    logger.info(f"\n{'='*70}")
    logger.info(f"ğŸ” TÃ¬m chunks chá»©a tá»« khÃ³a: '{keyword}'")
    logger.info(f"{'='*70}")
    logger.info(f"TÃ¬m tháº¥y: {len(matching_chunks)} chunks\n")
    
    for idx, content, metadata in matching_chunks[:3]:  # Hiá»ƒn thá»‹ top 3
        source = metadata.get('source', 'Unknown')
        page = metadata.get('page', 0)
        logger.info(f"[Chunk {idx}] Source: {source} - Page {page}")
        logger.info(f"Content: {content}...\n")
        logger.info("-" * 70 + "\n")

logger.info("\n" + "="*70)
logger.info("ğŸ“Š PHÃ‚N TÃCH:")
logger.info("="*70)
logger.info("â€¢ Náº¿u tá»« khÃ³a KHÃ”NG Ä‘Æ°á»£c tÃ¬m tháº¥y")
logger.info("  â†’ PDF cÃ³ thá»ƒ khÃ´ng chá»©a thÃ´ng tin Ä‘Ã³")
logger.info("  â†’ Hoáº·c dá»¯ liá»‡u á»Ÿ Ä‘á»‹nh dáº¡ng khÃ¡c (áº£nh, báº£ng, v.v.)")
logger.info("â€¢ Náº¿u chunks cÃ³ váº» bá»‹ cáº¯t giá»¯a chá»«ng")
logger.info("  â†’ Cáº§n Ä‘iá»u chá»‰nh CHUNK_SIZE hoáº·c separators")
logger.info("="*70)
