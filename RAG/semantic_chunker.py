from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from config import CHUNK_SIZE, CHUNK_OVERLAP
import logging

logger = logging.getLogger(__name__)

def semantic_chunk(documents: list) -> list:
    """
    T√°ch documents th√†nh chunks theo ng·ªØ nghƒ©a (semantic).
    S·ª≠ d·ª•ng RecursiveCharacterTextSplitter v·ªõi separators ph√π h·ª£p cho Ti·∫øng Vi·ªát.
    """
    logger.info("üî™ B·∫ÆT ƒê·∫¶U SEMANTIC CHUNKING")
    logger.info(f"   Chunk size: {CHUNK_SIZE} k√Ω t·ª±")
    logger.info(f"   Chunk overlap: {CHUNK_OVERLAP} k√Ω t·ª± ({int(CHUNK_OVERLAP/CHUNK_SIZE*100)}%)")
    
    # Separators theo th·ª© t·ª± ∆∞u ti√™n (semantic coherence)
    separators = [
        "\n\n",      # Ng·∫Øt ƒëo·∫°n vƒÉn (m·∫°nh nh·∫•t)
        "\n",        # Ng·∫Øt d√≤ng
        "„ÄÇ",        # D·∫•u ch·∫•m Trung Qu·ªëc (n·∫øu c√≥)
        "ÔºÅ",        # D·∫•u ch·∫•m than
        "Ôºü",        # D·∫•u ch·∫•m h·ªèi
        ".",         # D·∫•u ch·∫•m English
        " ",         # Kho·∫£ng tr·∫Øng
        ""           # Fallback: chia t·ª´
    ]
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=separators,
        length_function=len
    )
    
    chunks = []
    for i, doc in enumerate(documents):
        try:
            split_docs = text_splitter.split_documents([doc])
            chunks.extend(split_docs)
            
            if (i + 1) % 50 == 0:
                logger.info(f"   ƒê√£ x·ª≠ l√Ω: {i + 1}/{len(documents)} trang")
        
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è L·ªói trang {doc.metadata.get('page')}: {str(e)}")
            continue
    
    logger.info(f"‚úÖ T·∫°o th√†nh c√¥ng: {len(chunks)} chunks")
    logger.info("=" * 60 + "\n")
    
    return chunks
